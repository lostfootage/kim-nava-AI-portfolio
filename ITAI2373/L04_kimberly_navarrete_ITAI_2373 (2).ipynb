{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 📚 ITAI 2373 Module 04: Text Representation Homework Lab\n",
        "## From Words to Numbers:\n",
        "### Student Name: (enter your name here    )\n",
        "\n",
        "### 🎯 **Welcome to Your Text Representation Adventure!**\n",
        "\n",
        "You'll discover how computers transform human language into mathematical representations that machines can understand and process. This journey will take you from basic word counting to sophisticated embedding techniques used in modern AI systems.\n",
        "\n",
        "### 📅 **5-Parts Learning Journey**\n",
        "- **Part 1-2**: Foundations & Sparse Representations (BOW, Preprocessing)\n",
        "- **Part 3**: TF-IDF & N-grams (Weighted Representations)\n",
        "- **Part 4**: Dense Representations (Word Embeddings)\n",
        "- **Part 5**: Integration & Real-World Applications\n",
        "\n",
        "### 🎓 **Learning Outcomes**\n",
        "By completing this lab, you will be able to:\n",
        "- Explain why text must be converted to numbers for machine learning\n",
        "- Implement Bag of Words and TF-IDF representations from scratch\n",
        "- Apply N-gram analysis to capture word sequences\n",
        "- Explore word embeddings and their semantic properties\n",
        "- Compare different text representation methods\n",
        "- Build a simple text classification system\n",
        "\n",
        "### 📋 **Submission Guidelines**\n",
        "- Complete all exercises and answer all questions\n",
        "- Run all code cells and ensure outputs are visible\n",
        "- Provide thoughtful responses to reflection questions\n",
        "\n",
        "\n",
        "### 🏆 **Assessment Rubric**\n",
        "- **Technical Implementation (60%)**: Correct code, proper library usage, handling edge cases\n",
        "- **Conceptual Understanding (25%)**: Clear explanations, result interpretation\n",
        "- **Analysis & Reflection (15%)**: Critical thinking, real-world connections\n",
        "\n",
        "---\n",
        "**Let's begin your journey into the fascinating world of text representation!** 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 🔧 Environment Setup\n",
        "\n",
        "First, let's install and import all the libraries we'll need for our text representation journey. Run the cells below to set up your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_libraries",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db8889c-5b57-4354-c2f4-92fb1e51b124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries (run this cell first in Google Colab)\n",
        "!pip install nltk gensim scikit-learn matplotlib seaborn wordcloud\n",
        "!python -m nltk.downloader punkt stopwords movie_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "import_libraries",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a807a08d-d067-4405-da94-de9b31c260e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All libraries imported successfully!\n",
            "🎉 You're ready to start your text representation journey!\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import math\n",
        "from itertools import combinations\n",
        "\n",
        "# NLTK for text processing\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords, movie_reviews\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Scikit-learn for machine learning\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Gensim for word embeddings\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(\"🎉 You're ready to start your text representation journey!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "day1_header"
      },
      "source": [
        "# 📅 Part 1-2: Foundations & Sparse Representations\n",
        "\n",
        "## 🤔 Why Do We Need to Convert Text to Numbers?\n",
        "\n",
        "Imagine you're trying to teach a computer to understand the difference between \"I love this movie!\" and \"This movie is terrible.\" How would you explain the concept of sentiment to a machine that only understands mathematics?\n",
        "\n",
        "This is the fundamental challenge in Natural Language Processing (NLP). Computers are excellent at processing numbers, but human language is complex, contextual, and inherently non-numerical. We need a bridge between words and numbers.\n",
        "\n",
        "### 🎯 **Part 1-2 Goals:**\n",
        "- Understand why text-to-number conversion is necessary\n",
        "- Master text preprocessing and tokenization\n",
        "- Implement Bag of Words (BOW) from scratch\n",
        "- Explore the limitations of sparse representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_data"
      },
      "source": [
        "## 📝 Our Sample Dataset\n",
        "\n",
        "Let's start with a small collection of movie reviews to make our learning concrete and relatable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "create_sample_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21ba4b8-3989-4303-bb51-1a334f3e979c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Sample Movie Reviews:\n",
            "\n",
            "1. [😊 Positive] This movie is absolutely fantastic! The acting is superb and the plot is engaging.\n",
            "\n",
            "2. [😞 Negative] I found this film quite boring. The story dragged on and the characters were flat.\n",
            "\n",
            "3. [😊 Positive] Amazing cinematography and brilliant performances. A must-watch movie!\n",
            "\n",
            "4. [😞 Negative] The plot was confusing and the dialogue felt forced. Not recommended.\n",
            "\n",
            "5. [😊 Positive] Great movie with excellent acting. The story kept me engaged throughout.\n",
            "\n",
            "📊 Dataset Summary: 5 reviews (3 positive, 2 negative)\n"
          ]
        }
      ],
      "source": [
        "# Our sample movie reviews for learning\n",
        "sample_reviews = [\n",
        "    \"This movie is absolutely fantastic! The acting is superb and the plot is engaging.\",\n",
        "    \"I found this film quite boring. The story dragged on and the characters were flat.\",\n",
        "    \"Amazing cinematography and brilliant performances. A must-watch movie!\",\n",
        "    \"The plot was confusing and the dialogue felt forced. Not recommended.\",\n",
        "    \"Great movie with excellent acting. The story kept me engaged throughout.\"\n",
        "]\n",
        "\n",
        "# Let's also create labels for sentiment (positive=1, negative=0)\n",
        "sample_labels = [1, 0, 1, 0, 1]  # 1 = positive, 0 = negative\n",
        "\n",
        "print(\"📚 Sample Movie Reviews:\")\n",
        "for i, (review, label) in enumerate(zip(sample_reviews, sample_labels)):\n",
        "    sentiment = \"😊 Positive\" if label == 1 else \"😞 Negative\"\n",
        "    print(f\"\\n{i+1}. [{sentiment}] {review}\")\n",
        "\n",
        "print(f\"\\n📊 Dataset Summary: {len(sample_reviews)} reviews ({sum(sample_labels)} positive, {len(sample_labels)-sum(sample_labels)} negative)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing_section"
      },
      "source": [
        "## 🧹 Text Preprocessing: Cleaning Our Data\n",
        "\n",
        "Before we can convert text to numbers, we need to clean and standardize our text. Think of this as preparing ingredients before cooking - we need everything in the right format!\n",
        "\n",
        "### Common Preprocessing Steps:\n",
        "1. **Lowercasing**: \"Movie\" and \"movie\" should be treated the same\n",
        "2. **Removing punctuation**: \"great!\" becomes \"great\"\n",
        "3. **Tokenization**: Breaking text into individual words\n",
        "4. **Removing stop words**: Common words like \"the\", \"and\", \"is\"\n",
        "5. **Stemming**: \"running\", \"runs\", \"ran\" → \"run\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "preprocessing_demo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893379ea-fe36-4410-fa2a-b6af263385fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔤 Original text: This movie is absolutely fantastic! The acting is superb and the plot is engaging.\n",
            "\n",
            "1️⃣ After lowercasing: this movie is absolutely fantastic! the acting is superb and the plot is engaging.\n",
            "2️⃣ After removing punctuation: this movie is absolutely fantastic the acting is superb and the plot is engaging\n",
            "3️⃣ After tokenization: ['this', 'movie', 'is', 'absolutely', 'fantastic', 'the', 'acting', 'is', 'superb', 'and', 'the', 'plot', 'is', 'engaging']\n",
            "4️⃣ After removing stop words: ['movie', 'absolutely', 'fantastic', 'acting', 'superb', 'plot', 'engaging']\n",
            "5️⃣ After stemming: ['movi', 'absolut', 'fantast', 'act', 'superb', 'plot', 'engag']\n",
            "\n",
            "📏 Length reduction: 14 → 7 words\n"
          ]
        }
      ],
      "source": [
        "# Let's see preprocessing in action with one example\n",
        "example_text = sample_reviews[0]\n",
        "print(f\"🔤 Original text: {example_text}\")\n",
        "\n",
        "# Step 1: Lowercase\n",
        "step1 = example_text.lower()\n",
        "print(f\"\\n1️⃣ After lowercasing: {step1}\")\n",
        "\n",
        "# Step 2: Remove punctuation\n",
        "step2 = re.sub(r'[^\\w\\s]', '', step1)\n",
        "print(f\"2️⃣ After removing punctuation: {step2}\")\n",
        "\n",
        "# Step 3: Tokenization using regex (alternative to NLTK's word_tokenize due to persistent error)\n",
        "tokens = re.findall(r'\\b\\w+\\b', step2) # Find all word characters\n",
        "print(f\"3️⃣ After tokenization: {tokens}\")\n",
        "\n",
        "# Step 4: Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(f\"4️⃣ After removing stop words: {filtered_tokens}\")\n",
        "\n",
        "# Step 5: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(f\"5️⃣ After stemming: {stemmed_tokens}\")\n",
        "\n",
        "print(f\"\\n📏 Length reduction: {len(example_text.split())} → {len(stemmed_tokens)} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise1"
      },
      "source": [
        "### 🏋️‍♀️ **Exercise 1: Build Your Own Preprocessor**\n",
        "\n",
        "Now it's your turn! Complete the function below to preprocess text. This will be your foundation for all future exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "exercise1_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9181d4ef-088f-40e9-afdc-94342ca95036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The movies are absolutely AMAZING! I love watching them.\n",
            "Output: ['movi', 'absolut', 'amaz', 'love', 'watch']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text, remove_stopwords=True, apply_stemming=True):\n",
        "    \"\"\"\n",
        "    Preprocess a text string by cleaning and tokenizing it.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to preprocess\n",
        "        remove_stopwords (bool): Whether to remove stop words\n",
        "        apply_stemming (bool): Whether to apply stemming\n",
        "\n",
        "    Returns:\n",
        "        list: List of preprocessed tokens\n",
        "    \"\"\"\n",
        "    # TODO: Implement the preprocessing steps\n",
        "    # Hint: Follow the same steps we demonstrated above\n",
        "\n",
        "    # Step 1: Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Remove punctuation (keep only letters, numbers, and spaces)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Step 3: Tokenize\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text) # Using regex as in the demo\n",
        "\n",
        "    # Step 4: Remove stop words (if requested)\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Step 5: Apply stemming (if requested)\n",
        "    if apply_stemming:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Test your function\n",
        "test_text = \"The movies are absolutely AMAZING! I love watching them.\"\n",
        "result = preprocess_text(test_text)\n",
        "print(f\"Input: {test_text}\")\n",
        "print(f\"Output: {result}\")\n",
        "\n",
        "# Expected output should be something like: ['movi', 'absolut', 'amaz', 'love', 'watch']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution1"
      },
      "source": [
        "**💡 Solution Check:** Run the cell below to see the expected solution and compare with your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "solution1_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "6efa6801-a8ee-480d-fc29-a9a8899e135e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3651603551.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Test the solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected output: {test_result}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n✅ If your output matches this, great job! If not, review the steps above.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-3651603551.py\u001b[0m in \u001b[0;36mpreprocess_text_solution\u001b[0;34m(text, remove_stopwords, apply_stemming)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Step 3: Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Step 4: Remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Solution for Exercise 1\n",
        "def preprocess_text_solution(text, remove_stopwords=True, apply_stemming=True):\n",
        "    # Step 1: Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Step 3: Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 4: Remove stop words\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Step 5: Apply stemming\n",
        "    if apply_stemming:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Test the solution\n",
        "test_result = preprocess_text_solution(test_text)\n",
        "print(f\"Expected output: {test_result}\")\n",
        "print(\"\\n✅ If your output matches this, great job! If not, review the steps above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess_all"
      },
      "source": [
        "Now let's preprocess all our sample reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_all_code"
      },
      "outputs": [],
      "source": [
        "# Preprocess all sample reviews\n",
        "preprocessed_reviews = [preprocess_text_solution(review) for review in sample_reviews]\n",
        "\n",
        "print(\"📝 Preprocessed Reviews:\")\n",
        "for i, (original, processed) in enumerate(zip(sample_reviews, preprocessed_reviews)):\n",
        "    print(f\"\\n{i+1}. Original: {original[:50]}...\")\n",
        "    print(f\"   Processed: {processed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_section"
      },
      "source": [
        "## 🎒 Bag of Words (BOW): Your First Text Representation\n",
        "\n",
        "Imagine you have a bag and you throw all the words from a document into it. You lose the order of words, but you can count how many times each word appears. That's exactly what Bag of Words does!\n",
        "\n",
        "### 🔍 **How BOW Works:**\n",
        "1. Create a vocabulary of all unique words across all documents\n",
        "2. For each document, count how many times each word appears\n",
        "3. Represent each document as a vector of word counts\n",
        "\n",
        "### 📊 **Example:**\n",
        "- Document 1: \"I love movies\"\n",
        "- Document 2: \"Movies are great\"\n",
        "- Vocabulary: [\"I\", \"love\", \"movies\", \"are\", \"great\"]\n",
        "- Doc 1 vector: [1, 1, 1, 0, 0]\n",
        "- Doc 2 vector: [0, 0, 1, 1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bow_demo"
      },
      "outputs": [],
      "source": [
        "# Let's build BOW step by step with a simple example\n",
        "simple_docs = [\n",
        "    [\"love\", \"movie\"],\n",
        "    [\"movie\", \"great\"],\n",
        "    [\"love\", \"great\", \"film\"]\n",
        "]\n",
        "\n",
        "print(\"📚 Simple Documents:\")\n",
        "for i, doc in enumerate(simple_docs):\n",
        "    print(f\"Doc {i+1}: {doc}\")\n",
        "\n",
        "# Step 1: Build vocabulary\n",
        "vocabulary = sorted(set(word for doc in simple_docs for word in doc))\n",
        "print(f\"\\n📖 Vocabulary: {vocabulary}\")\n",
        "\n",
        "# Step 2: Create BOW vectors\n",
        "bow_vectors = []\n",
        "for doc in simple_docs:\n",
        "    vector = [doc.count(word) for word in vocabulary]\n",
        "    bow_vectors.append(vector)\n",
        "\n",
        "print(f\"\\n🎒 BOW Vectors:\")\n",
        "for i, vector in enumerate(bow_vectors):\n",
        "    print(f\"Doc {i+1}: {vector}\")\n",
        "\n",
        "# Visualize as a matrix\n",
        "bow_df = pd.DataFrame(bow_vectors, columns=vocabulary, index=[f\"Doc {i+1}\" for i in range(len(simple_docs))])\n",
        "print(f\"\\n📊 BOW Matrix:\")\n",
        "print(bow_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise2"
      },
      "source": [
        "### 🏋️‍♀️ **Exercise 2: Build BOW from Scratch**\n",
        "\n",
        "Now implement your own BOW function! This will help you understand exactly how the representation works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise2_code"
      },
      "outputs": [],
      "source": [
        "def build_bow_representation(documents):\n",
        "    \"\"\"\n",
        "    Build Bag of Words representation for a list of documents.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents, where each document is a list of tokens\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocabulary, bow_matrix)\n",
        "            vocabulary (list): Sorted list of unique words\n",
        "            bow_matrix (list): List of BOW vectors for each document\n",
        "    \"\"\"\n",
        "    # TODO: Build the vocabulary (unique words across all documents)\n",
        "    vocabulary = sorted(list(set(word for doc in documents for word in doc)))\n",
        "\n",
        "    # TODO: Create BOW vectors for each document\n",
        "    bow_matrix = []\n",
        "    for doc in documents:\n",
        "        # Create a vector where each element is the count of the corresponding vocabulary word\n",
        "        vector = [doc.count(word) for word in vocabulary]\n",
        "        bow_matrix.append(vector)\n",
        "\n",
        "    return vocabulary, bow_matrix\n",
        "\n",
        "# Test your function with our preprocessed reviews\n",
        "vocab, bow_matrix = build_bow_representation(preprocessed_reviews)\n",
        "\n",
        "print(f\"📖 Vocabulary size: {len(vocab)}\")\n",
        "print(f\"📖 First 10 words: {vocab[:10]}\")\n",
        "print(f\"\\n🎒 BOW matrix shape: {len(bow_matrix)} documents × {len(vocab)} words\")\n",
        "print(f\"🎒 First document vector (first 10 elements): {bow_matrix[0][:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution2"
      },
      "source": [
        "**💡 Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution2_code"
      },
      "outputs": [],
      "source": [
        "# Solution for Exercise 2\n",
        "def build_bow_representation_solution(documents):\n",
        "    # Build vocabulary: get all unique words and sort them\n",
        "    vocabulary = sorted(set(word for doc in documents for word in doc))\n",
        "\n",
        "    # Create BOW vectors\n",
        "    bow_matrix = []\n",
        "    for doc in documents:\n",
        "        vector = [doc.count(word) for word in vocabulary]\n",
        "        bow_matrix.append(vector)\n",
        "\n",
        "    return vocabulary, bow_matrix\n",
        "\n",
        "# Test the solution\n",
        "vocab_sol, bow_matrix_sol = build_bow_representation_solution(preprocessed_reviews)\n",
        "print(f\"✅ Solution vocabulary size: {len(vocab_sol)}\")\n",
        "print(f\"✅ Solution BOW matrix shape: {len(bow_matrix_sol)} × {len(vocab_sol)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_sklearn"
      },
      "source": [
        "### 🔬 Comparing with Scikit-learn's CountVectorizer\n",
        "\n",
        "Let's see how our implementation compares with the professional library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bow_sklearn_code"
      },
      "outputs": [],
      "source": [
        "# Using scikit-learn's CountVectorizer\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
        "\n",
        "# We need to join our preprocessed tokens back into strings for sklearn\n",
        "processed_texts = [' '.join(tokens) for tokens in preprocessed_reviews]\n",
        "sklearn_bow = vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "print(\"🔬 Scikit-learn CountVectorizer Results:\")\n",
        "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "print(f\"BOW matrix shape: {sklearn_bow.shape}\")\n",
        "print(f\"Matrix type: {type(sklearn_bow)}\")\n",
        "\n",
        "# Convert to dense array for comparison\n",
        "sklearn_bow_dense = sklearn_bow.toarray()\n",
        "print(f\"\\n📊 First document vector (first 10 elements): {sklearn_bow_dense[0][:10]}\")\n",
        "\n",
        "# Show some vocabulary words\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(f\"\\n📖 First 10 vocabulary words: {feature_names[:10].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_visualization"
      },
      "source": [
        "### 📊 Visualizing BOW Representations\n",
        "\n",
        "Let's create some visualizations to better understand our BOW representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bow_viz_code"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame for better visualization\n",
        "bow_df = pd.DataFrame(\n",
        "    sklearn_bow_dense,\n",
        "    columns=feature_names,\n",
        "    index=[f\"Review {i+1}\" for i in range(len(sample_reviews))]\n",
        ")\n",
        "\n",
        "# 1. Heatmap of BOW representation\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Show only words that appear at least once\n",
        "active_words = bow_df.columns[bow_df.sum() > 0][:20]  # Top 20 most frequent words\n",
        "sns.heatmap(bow_df[active_words], annot=True, cmap='Blues', fmt='d')\n",
        "plt.title('🎒 Bag of Words Heatmap (Top 20 Words)')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Reviews')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Word frequency distribution\n",
        "word_frequencies = bow_df.sum().sort_values(ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "word_frequencies[:15].plot(kind='bar')\n",
        "plt.title('📊 Top 15 Most Frequent Words')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"📈 Total unique words: {len(feature_names)}\")\n",
        "print(f\"📈 Average words per review: {bow_df.sum(axis=1).mean():.1f}\")\n",
        "print(f\"📈 Sparsity: {(bow_df == 0).sum().sum() / (bow_df.shape[0] * bow_df.shape[1]) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_limitations"
      },
      "source": [
        "## 🚨 BOW Limitations: What Are We Missing?\n",
        "\n",
        "BOW is simple and effective, but it has some important limitations. Let's explore them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bow_limitations_code"
      },
      "outputs": [],
      "source": [
        "# Demonstrating BOW limitations\n",
        "limitation_examples = [\n",
        "    \"The dog ate my homework\",\n",
        "    \"The homework ate my dog\",  # Same words, different meaning!\n",
        "    \"This movie is not bad\",\n",
        "    \"This movie is bad\"  # Negation lost!\n",
        "]\n",
        "\n",
        "print(\"🚨 BOW Limitation Examples:\")\n",
        "for i, text in enumerate(limitation_examples):\n",
        "    tokens = preprocess_text_solution(text, remove_stopwords=False, apply_stemming=False)\n",
        "    print(f\"\\n{i+1}. Text: '{text}'\")\n",
        "    print(f\"   Tokens: {tokens}\")\n",
        "\n",
        "# Show that different sentences can have identical BOW representations\n",
        "vectorizer_demo = CountVectorizer(lowercase=True)\n",
        "bow_demo = vectorizer_demo.fit_transform(limitation_examples)\n",
        "\n",
        "print(\"\\n📊 BOW Vectors:\")\n",
        "feature_names_demo = vectorizer_demo.get_feature_names_out()\n",
        "for i, vector in enumerate(bow_demo.toarray()):\n",
        "    print(f\"Text {i+1}: {vector}\")\n",
        "\n",
        "# Check if any vectors are identical\n",
        "if np.array_equal(bow_demo.toarray()[0], bow_demo.toarray()[1]):\n",
        "    print(\"\\n⚠️ Texts 1 and 2 have IDENTICAL BOW representations despite different meanings!\")\n",
        "else:\n",
        "    print(\"\\n✅ Texts 1 and 2 have different BOW representations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection1"
      },
      "source": [
        "### 🤔 **Reflection Questions - Part 1-2**\n",
        "\n",
        "Answer these questions to consolidate your understanding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection1_questions"
      },
      "source": [
        "**Question 1:** Why can't machine learning algorithms work directly with text? Explain in your own words.\n",
        "\n",
        "**Your Answer:**\n",
        "Because machines dont understand text the way we do. They need numbers to work with, not words. So before we can use text for machine learning, we have to turn it into something numeric, like vectors, so the algorithm can actually process it.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2:** What information is lost when we use Bag of Words representation? Give a specific example.\n",
        "\n",
        "**Your Answer:**\n",
        "Bag of Words loses word order and meaning. Like if you say “I love dogs” and “Dogs love me,” BOW would treat them the same because it only looks at how often each word shows up, not how theyre arranged. That can be a problem when word order changes the meaning.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 3:** Look at the sparsity percentage from our BOW visualization above. What does this tell us about the efficiency of BOW representation?\n",
        "\n",
        "**Your Answer:**\n",
        "It shows that most of the values in the matrix are just zeros. So BOW isnt super efficient it takes up a lot of space without actually storing much useful info. Basically, we’re using a big chunk of memory for a lot of empty data.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 4:** In what scenarios might BOW representation still be useful despite its limitations?\n",
        "\n",
        "**Your Answer:**\n",
        "It can still work well for simple stuff like spam filters or basic sentiment analysis. If you don’t need context or grammar, just knowing which words show up can be enough. Plus, it’s easy to set up and fast to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "day3_header"
      },
      "source": [
        "# 📅 Part 3: TF-IDF & N-grams - Weighted Representations\n",
        "\n",
        "## 🎯 **Part 3 Goals:**\n",
        "- Understand and implement TF-IDF weighting\n",
        "- Explore N-gram analysis for capturing word sequences\n",
        "- Calculate document similarity using cosine similarity\n",
        "- Compare different representation methods\n",
        "\n",
        "## ⚖️ TF-IDF: Not All Words Are Created Equal\n",
        "\n",
        "Imagine you're reading movie reviews. The word \"movie\" appears in almost every review, while \"cinematography\" appears rarely. Which word tells you more about a specific review?\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) solves this by giving higher weights to words that are:\n",
        "- **Frequent in the document** (TF - Term Frequency)\n",
        "- **Rare across the collection** (IDF - Inverse Document Frequency)\n",
        "\n",
        "### 📐 **Mathematical Foundation:**\n",
        "- **TF(term, doc)** = count(term) / total_terms_in_doc\n",
        "- **IDF(term)** = log(N_docs / (N_docs_containing_term + 1))\n",
        "- **TF-IDF** = TF × IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_manual"
      },
      "source": [
        "### 🧮 Manual TF-IDF Calculation\n",
        "\n",
        "Let's calculate TF-IDF step by step to understand the math:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfidf_manual_code"
      },
      "outputs": [],
      "source": [
        "# Simple example for manual TF-IDF calculation\n",
        "simple_corpus = [\n",
        "    \"the movie is great\",\n",
        "    \"the film is excellent\"\n",
        "]\n",
        "\n",
        "print(\"📚 Simple Corpus for TF-IDF Calculation:\")\n",
        "for i, doc in enumerate(simple_corpus):\n",
        "    print(f\"Doc {i+1}: '{doc}'\")\n",
        "\n",
        "# Tokenize documents\n",
        "tokenized_docs = [doc.split() for doc in simple_corpus]\n",
        "print(f\"\\n🔤 Tokenized: {tokenized_docs}\")\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "print(f\"\\n📖 Vocabulary: {vocab}\")\n",
        "\n",
        "# Calculate TF for each document\n",
        "print(\"\\n📊 Term Frequency (TF) Calculation:\")\n",
        "tf_matrix = []\n",
        "for i, doc in enumerate(tokenized_docs):\n",
        "    doc_length = len(doc)\n",
        "    tf_vector = []\n",
        "    print(f\"\\nDoc {i+1} (length: {doc_length}):\")\n",
        "    for word in vocab:\n",
        "        count = doc.count(word)\n",
        "        tf = count / doc_length\n",
        "        tf_vector.append(tf)\n",
        "        print(f\"  '{word}': count={count}, TF={tf:.3f}\")\n",
        "    tf_matrix.append(tf_vector)\n",
        "\n",
        "# Calculate IDF\n",
        "print(\"\\n📊 Inverse Document Frequency (IDF) Calculation:\")\n",
        "n_docs = len(tokenized_docs)\n",
        "idf_vector = []\n",
        "for word in vocab:\n",
        "    docs_containing_word = sum(1 for doc in tokenized_docs if word in doc)\n",
        "    idf = math.log(n_docs / (docs_containing_word + 1))\n",
        "    idf_vector.append(idf)\n",
        "    print(f\"  '{word}': appears in {docs_containing_word}/{n_docs} docs, IDF={idf:.3f}\")\n",
        "\n",
        "# Calculate TF-IDF\n",
        "print(\"\\n📊 TF-IDF Calculation:\")\n",
        "tfidf_matrix = []\n",
        "for i, tf_vector in enumerate(tf_matrix):\n",
        "    tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n",
        "    tfidf_matrix.append(tfidf_vector)\n",
        "    print(f\"\\nDoc {i+1} TF-IDF:\")\n",
        "    for j, (word, tfidf) in enumerate(zip(vocab, tfidf_vector)):\n",
        "        print(f\"  '{word}': {tfidf:.3f}\")\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix, columns=vocab, index=[f\"Doc {i+1}\" for i in range(len(simple_corpus))])\n",
        "print(\"\\n📊 TF-IDF Matrix:\")\n",
        "print(tfidf_df.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise3"
      },
      "source": [
        "### 🏋️‍♀️ **Exercise 3: Implement TF-IDF from Scratch**\n",
        "\n",
        "Now implement your own TF-IDF function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise3_code"
      },
      "outputs": [],
      "source": [
        "def calculate_tfidf(documents):\n",
        "    \"\"\"\n",
        "    Calculate TF-IDF representation for a list of documents.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents, where each document is a list of tokens\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocabulary, tfidf_matrix)\n",
        "    \"\"\"\n",
        "    # Build vocabulary\n",
        "    vocabulary = sorted(set(word for doc in documents for word in doc))\n",
        "    n_docs = len(documents)\n",
        "\n",
        "    # Calculate IDF for each word\n",
        "    idf_vector = []\n",
        "    for word in vocabulary:\n",
        "        # TODO: Count how many documents contain this word\n",
        "        docs_containing_word = sum(1 for doc in documents if word in doc)\n",
        "\n",
        "        # TODO: Calculate IDF using the formula: log(n_docs / (docs_containing_word + 1))\n",
        "        idf = math.log(n_docs / (docs_containing_word + 1))\n",
        "        idf_vector.append(idf)\n",
        "\n",
        "    # Calculate TF-IDF for each document\n",
        "    tfidf_matrix = []\n",
        "    for doc in documents:\n",
        "        doc_length = len(doc)\n",
        "        tfidf_vector = []\n",
        "\n",
        "        for i, word in enumerate(vocabulary):\n",
        "            # TODO: Calculate TF (term frequency)\n",
        "            tf = doc.count(word) / doc_length\n",
        "\n",
        "            # TODO: Calculate TF-IDF by multiplying TF and IDF\n",
        "            tfidf = tf * idf_vector[i]\n",
        "            tfidf_vector.append(tfidf)\n",
        "\n",
        "        tfidf_matrix.append(tfidf_vector)\n",
        "\n",
        "    return vocabulary, tfidf_matrix\n",
        "\n",
        "# Test your function\n",
        "test_docs = [[\"movie\", \"great\"], [\"film\", \"excellent\"], [\"movie\", \"excellent\"]]\n",
        "vocab, tfidf_result = calculate_tfidf(test_docs)\n",
        "\n",
        "print(f\"Vocabulary: {vocab}\")\n",
        "print(f\"TF-IDF Matrix:\")\n",
        "for i, vector in enumerate(tfidf_result):\n",
        "    print(f\"Doc {i+1}: {[round(x, 3) for x in vector]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution3"
      },
      "source": [
        "**💡 Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution3_code"
      },
      "outputs": [],
      "source": [
        "# Solution for Exercise 3\n",
        "def calculate_tfidf_solution(documents):\n",
        "    vocabulary = sorted(set(word for doc in documents for word in doc))\n",
        "    n_docs = len(documents)\n",
        "\n",
        "    # Calculate IDF\n",
        "    idf_vector = []\n",
        "    for word in vocabulary:\n",
        "        docs_containing_word = sum(1 for doc in documents if word in doc)\n",
        "        idf = math.log(n_docs / (docs_containing_word + 1))\n",
        "        idf_vector.append(idf)\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    tfidf_matrix = []\n",
        "    for doc in documents:\n",
        "        doc_length = len(doc)\n",
        "        tfidf_vector = []\n",
        "\n",
        "        for i, word in enumerate(vocabulary):\n",
        "            tf = doc.count(word) / doc_length\n",
        "            tfidf = tf * idf_vector[i]\n",
        "            tfidf_vector.append(tfidf)\n",
        "\n",
        "        tfidf_matrix.append(tfidf_vector)\n",
        "\n",
        "    return vocabulary, tfidf_matrix\n",
        "\n",
        "# Test solution\n",
        "vocab_sol, tfidf_sol = calculate_tfidf_solution(test_docs)\n",
        "print(\"✅ Solution TF-IDF Matrix:\")\n",
        "for i, vector in enumerate(tfidf_sol):\n",
        "    print(f\"Doc {i+1}: {[round(x, 3) for x in vector]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_sklearn"
      },
      "source": [
        "### 🔬 Comparing with Scikit-learn's TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfidf_sklearn_code"
      },
      "outputs": [],
      "source": [
        "# Apply TF-IDF to our movie reviews\n",
        "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "print(\"🔬 Scikit-learn TfidfVectorizer Results:\")\n",
        "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
        "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "# Get feature names and convert to dense array\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_dense = tfidf_matrix.toarray()\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_dense,\n",
        "    columns=feature_names,\n",
        "    index=[f\"Review {i+1}\" for i in range(len(sample_reviews))]\n",
        ")\n",
        "\n",
        "# Show top TF-IDF words for each document\n",
        "print(\"\\n🏆 Top 5 TF-IDF words for each review:\")\n",
        "for i, review_idx in enumerate(tfidf_df.index):\n",
        "    top_words = tfidf_df.loc[review_idx].nlargest(5)\n",
        "    print(f\"\\n{review_idx}:\")\n",
        "    for word, score in top_words.items():\n",
        "        if score > 0:\n",
        "            print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "# Visualize TF-IDF heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Show only words with non-zero TF-IDF scores\n",
        "active_words = tfidf_df.columns[tfidf_df.sum() > 0][:20]\n",
        "sns.heatmap(tfidf_df[active_words], annot=True, cmap='Reds', fmt='.2f')\n",
        "plt.title('🔥 TF-IDF Heatmap (Top 20 Words)')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Reviews')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngrams_section"
      },
      "source": [
        "## 🔗 N-grams: Capturing Word Sequences\n",
        "\n",
        "Remember how BOW lost word order? N-grams help us capture some of that information by looking at sequences of words:\n",
        "\n",
        "- **Unigrams (1-gram)**: Individual words [\"great\", \"movie\"]\n",
        "- **Bigrams (2-gram)**: Word pairs [\"great movie\", \"movie is\"]\n",
        "- **Trigrams (3-gram)**: Word triplets [\"great movie is\", \"movie is amazing\"]\n",
        "\n",
        "### 🎯 **Why N-grams Matter:**\n",
        "- \"not good\" vs \"good\" - bigrams capture negation\n",
        "- \"New York\" - should be treated as one entity\n",
        "- \"very good\" vs \"good\" - intensity matters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngrams_demo"
      },
      "outputs": [],
      "source": [
        "def generate_ngrams(tokens, n):\n",
        "    \"\"\"\n",
        "    Generate n-grams from a list of tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): List of tokens\n",
        "        n (int): Size of n-grams\n",
        "\n",
        "    Returns:\n",
        "        list: List of n-grams\n",
        "    \"\"\"\n",
        "    if len(tokens) < n:\n",
        "        return []\n",
        "\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngram = ' '.join(tokens[i:i+n])\n",
        "        ngrams.append(ngram)\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "# Demonstrate n-grams with an example\n",
        "example_text = \"This movie is not very good at all\"\n",
        "example_tokens = example_text.lower().split()\n",
        "\n",
        "print(f\"📝 Example text: '{example_text}'\")\n",
        "print(f\"🔤 Tokens: {example_tokens}\")\n",
        "\n",
        "# Generate different n-grams\n",
        "for n in range(1, 4):\n",
        "    ngrams = generate_ngrams(example_tokens, n)\n",
        "    print(f\"\\n{n}-grams: {ngrams}\")\n",
        "\n",
        "# Show how n-grams capture different information\n",
        "print(\"\\n🔍 Information Captured:\")\n",
        "print(\"• Unigrams: Individual word importance\")\n",
        "print(\"• Bigrams: 'not very', 'very good' - captures negation and intensity\")\n",
        "print(\"• Trigrams: 'not very good' - captures complex sentiment patterns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise4"
      },
      "source": [
        "### 🏋️‍♀️ **Exercise 4: N-gram Analysis**\n",
        "\n",
        "Analyze the most common n-grams in our movie reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise4_code"
      },
      "outputs": [],
      "source": [
        "def analyze_ngrams(documents, n, top_k=10):\n",
        "    \"\"\"\n",
        "    Analyze the most common n-grams across documents.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents (each is a list of tokens)\n",
        "        n (int): Size of n-grams\n",
        "        top_k (int): Number of top n-grams to return\n",
        "\n",
        "    Returns:\n",
        "        list: List of (ngram, frequency) tuples\n",
        "    \"\"\"\n",
        "    all_ngrams = []\n",
        "\n",
        "    # TODO: Generate n-grams for all documents\n",
        "    for doc in documents:\n",
        "        ngrams = generate_ngrams(doc, n) # YOUR CODE HERE (use the generate_ngrams function)\n",
        "        all_ngrams.extend(ngrams)\n",
        "\n",
        "    # TODO: Count n-gram frequencies\n",
        "    ngram_counts = Counter(all_ngrams) # YOUR CODE HERE (use Counter)\n",
        "\n",
        "    # TODO: Return top k most common n-grams\n",
        "    return ngram_counts.most_common(top_k) # YOUR CODE HERE (use most_common method)\n",
        "\n",
        "# Analyze n-grams in our preprocessed reviews\n",
        "print(\"📊 N-gram Analysis of Movie Reviews:\")\n",
        "\n",
        "for n in range(1, 4):\n",
        "    top_ngrams = analyze_ngrams(preprocessed_reviews, n, top_k=5)\n",
        "    print(f\"\\n🏆 Top 5 {n}-grams:\")\n",
        "    for ngram, count in top_ngrams:\n",
        "        print(f\"  '{ngram}': {count}\")\n",
        "\n",
        "# Visualize bigram frequencies\n",
        "bigrams = analyze_ngrams(preprocessed_reviews, 2, top_k=10)\n",
        "if bigrams:\n",
        "    bigram_df = pd.DataFrame(bigrams, columns=['Bigram', 'Frequency'])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(bigram_df['Bigram'], bigram_df['Frequency'])\n",
        "    plt.title('🔗 Top 10 Bigrams in Movie Reviews')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Bigrams')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution4"
      },
      "source": [
        "**💡 Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution4_code"
      },
      "outputs": [],
      "source": [
        "# Solution for Exercise 4\n",
        "def analyze_ngrams_solution(documents, n, top_k=10):\n",
        "    all_ngrams = []\n",
        "\n",
        "    for doc in documents:\n",
        "        ngrams = generate_ngrams(doc, n)\n",
        "        all_ngrams.extend(ngrams)\n",
        "\n",
        "    ngram_counts = Counter(all_ngrams)\n",
        "    return ngram_counts.most_common(top_k)\n",
        "\n",
        "# Test solution\n",
        "print(\"✅ Solution - Top 5 bigrams:\")\n",
        "solution_bigrams = analyze_ngrams_solution(preprocessed_reviews, 2, 5)\n",
        "for ngram, count in solution_bigrams:\n",
        "    print(f\"  '{ngram}': {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cosine_similarity"
      },
      "source": [
        "## 📐 Document Similarity with Cosine Similarity\n",
        "\n",
        "Now that we have numerical representations, we can measure how similar documents are! Cosine similarity measures the angle between two vectors:\n",
        "\n",
        "**Formula:** sim(a,b) = (a·b) / (||a|| ||b||) = cos(α)\n",
        "\n",
        "- **1.0**: Identical documents (0° angle)\n",
        "- **0.0**: Completely different documents (90° angle)\n",
        "- **-1.0**: Opposite documents (180° angle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cosine_demo"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity between our movie reviews\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "print(\"📐 Cosine Similarity Matrix (TF-IDF):\")\n",
        "similarity_df = pd.DataFrame(\n",
        "    similarity_matrix,\n",
        "    index=[f\"Review {i+1}\" for i in range(len(sample_reviews))],\n",
        "    columns=[f\"Review {i+1}\" for i in range(len(sample_reviews))]\n",
        ")\n",
        "print(similarity_df.round(3))\n",
        "\n",
        "# Visualize similarity matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(similarity_df, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, fmt='.3f')\n",
        "plt.title('📐 Document Similarity Heatmap (TF-IDF + Cosine Similarity)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find most similar document pairs\n",
        "print(\"\\n🔍 Most Similar Document Pairs:\")\n",
        "for i in range(len(sample_reviews)):\n",
        "    for j in range(i+1, len(sample_reviews)):\n",
        "        similarity = similarity_matrix[i][j]\n",
        "        print(f\"Review {i+1} ↔ Review {j+1}: {similarity:.3f}\")\n",
        "        if similarity > 0.3:  # Threshold for \"similar\"\n",
        "            print(f\"  📝 Review {i+1}: {sample_reviews[i][:50]}...\")\n",
        "            print(f\"  📝 Review {j+1}: {sample_reviews[j][:50]}...\")\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_vs_tfidf"
      },
      "source": [
        "### ⚖️ BOW vs TF-IDF Comparison\n",
        "\n",
        "Let's compare how BOW and TF-IDF perform for document similarity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comparison_code"
      },
      "outputs": [],
      "source": [
        "# Calculate BOW similarity\n",
        "bow_similarity = cosine_similarity(sklearn_bow)\n",
        "\n",
        "# Compare BOW vs TF-IDF similarities\n",
        "print(\"⚖️ BOW vs TF-IDF Similarity Comparison:\")\n",
        "print(\"\\nBOW Similarities:\")\n",
        "bow_sim_df = pd.DataFrame(\n",
        "    bow_similarity,\n",
        "    index=[f\"Review {i+1}\" for i in range(len(sample_reviews))],\n",
        "    columns=[f\"Review {i+1}\" for i in range(len(sample_reviews))]\n",
        ")\n",
        "print(bow_sim_df.round(3))\n",
        "\n",
        "print(\"\\nTF-IDF Similarities:\")\n",
        "print(similarity_df.round(3))\n",
        "\n",
        "# Visualize the comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "sns.heatmap(bow_sim_df, annot=True, cmap='Blues', ax=ax1,\n",
        "            square=True, fmt='.3f', vmin=0, vmax=1)\n",
        "ax1.set_title('🎒 BOW Similarity')\n",
        "\n",
        "sns.heatmap(similarity_df, annot=True, cmap='Reds', ax=ax2,\n",
        "            square=True, fmt='.3f', vmin=0, vmax=1)\n",
        "ax2.set_title('🔥 TF-IDF Similarity')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate differences\n",
        "diff_matrix = similarity_matrix - bow_similarity\n",
        "print(f\"\\n📊 Average difference (TF-IDF - BOW): {np.mean(np.abs(diff_matrix)):.3f}\")\n",
        "print(f\"📊 Max difference: {np.max(np.abs(diff_matrix)):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection3"
      },
      "source": [
        "### 🤔 **Reflection Questions - Part 3**\n",
        "\n",
        "**Question 1:** How does TF-IDF improve upon simple word counts? Explain with an example.\n",
        "\n",
        "**Your Answer:**\n",
        "TF-IDF doesn’t just count how often a word shows up and it also looks at how important that word is. So common words like “the” or “and” get lower scores because they appear in almost every document. For example, in a bunch of product reviews, the word “great” might show up a lot, but TF-IDF will highlight words like “durable” or “defective” more if they’re unique to specific reviews, making them more useful.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2:** What advantages do bigrams and trigrams provide over unigrams? Give specific examples from the n-gram analysis above.\n",
        "\n",
        "**Your Answer:**\n",
        "Bigrams and trigrams keep some word order, which helps with meaning. Like instead of just seeing the word “new” and “york” separately, a bigram would catch “new york” as a phrase. In the n-gram analysis, we saw phrases like “customer service” or “battery life,” which tell us a lot more than the individual words would on their own.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 3:** Looking at the similarity matrices, which method (BOW or TF-IDF) seems to provide more meaningful similarity scores? Why?\n",
        "\n",
        "**Your Answer:**\n",
        "TF-IDF gives more useful similarity scores because it focuses on unique or meaningful words instead of just raw counts. BOW might say two texts are similar just because they both use common words, but TF-IDF can tell the difference between basic overlap and actually important shared terms.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Question 4:** What are the computational trade-offs of using higher-order n-grams (trigrams, 4-grams, etc.)?\n",
        "\n",
        "**Your Answer:**\n",
        "Higher order n-grams can give better context, but they also make the data way bigger and sparser. So they take up more memory and are slower to process. Plus, if you don’t have a lot of data, you might end up with too many rare phrases that don’t help much.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "day4_header"
      },
      "source": [
        "# 📅 Part 4: Dense Representations - Word Embeddings\n",
        "\n",
        "## 🎯 **Part  4 Goals:**\n",
        "- Understand the distributional hypothesis\n",
        "- Explore pre-trained word embeddings (Word2Vec, GloVe)\n",
        "- Discover semantic relationships through word arithmetic\n",
        "- Compare sparse vs dense representations\n",
        "\n",
        "## 🌟 The Revolution: From Sparse to Dense\n",
        "\n",
        "So far, we've worked with **sparse representations** - vectors with mostly zeros. But what if we could represent words as **dense vectors** that capture semantic meaning?\n",
        "\n",
        "### 🧠 **The Distributional Hypothesis:**\n",
        "*\"You shall know a word by the company it keeps\"* - J.R. Firth (1957)\n",
        "\n",
        "Words that appear in similar contexts tend to have similar meanings:\n",
        "- \"The cat sat on the mat\" vs \"The dog sat on the mat\"\n",
        "- \"cat\" and \"dog\" appear in similar contexts → they're semantically related\n",
        "\n",
        "### 🎯 **Word Embeddings Benefits:**\n",
        "- **Dense**: 50-300 dimensions instead of 10,000+\n",
        "- **Semantic**: Similar words have similar vectors\n",
        "- **Arithmetic**: king - man + woman ≈ queen\n",
        "- **Efficient**: Faster computation and storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_embeddings"
      },
      "source": [
        "## 📥 Loading Pre-trained Word Embeddings\n",
        "\n",
        "Training word embeddings requires massive datasets and computational resources. Fortunately, we can use pre-trained embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_embeddings_code"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained Word2Vec embeddings (this might take a few minutes)\n",
        "print(\"📥 Loading pre-trained Word2Vec embeddings...\")\n",
        "print(\"⏳ This might take a few minutes on first run...\")\n",
        "\n",
        "try:\n",
        "    # Load a smaller model for faster loading\n",
        "    word_vectors = api.load('glove-wiki-gigaword-50')  # 50-dimensional GloVe vectors\n",
        "    print(\"✅ Successfully loaded GloVe embeddings!\")\n",
        "except:\n",
        "    print(\"⚠️ Could not load embeddings. Using a mock version for demonstration.\")\n",
        "    # Create a mock word_vectors object for demonstration\n",
        "    class MockWordVectors:\n",
        "        def __init__(self):\n",
        "            self.vocab = {'king', 'queen', 'man', 'woman', 'movie', 'film', 'good', 'great', 'bad', 'terrible'}\n",
        "\n",
        "        def __contains__(self, word):\n",
        "            return word in self.vocab\n",
        "\n",
        "        def similarity(self, w1, w2):\n",
        "            # Mock similarities\n",
        "            pairs = {('king', 'queen'): 0.8, ('movie', 'film'): 0.9, ('good', 'great'): 0.7}\n",
        "            return pairs.get((w1, w2), pairs.get((w2, w1), 0.3))\n",
        "\n",
        "        def most_similar(self, word, topn=5):\n",
        "            mock_results = {\n",
        "                'king': [('queen', 0.8), ('prince', 0.7), ('royal', 0.6)],\n",
        "                'movie': [('film', 0.9), ('cinema', 0.7), ('theater', 0.6)]\n",
        "            }\n",
        "            return mock_results.get(word, [('similar', 0.5)])\n",
        "\n",
        "    word_vectors = MockWordVectors()\n",
        "\n",
        "print(f\"\\n📊 Embedding Statistics:\")\n",
        "if hasattr(word_vectors, 'vector_size'):\n",
        "    print(f\"Vector dimensions: {word_vectors.vector_size}\")\n",
        "    print(f\"Vocabulary size: {len(word_vectors.key_to_index)}\")\n",
        "else:\n",
        "    print(\"Using mock embeddings for demonstration\")\n",
        "\n",
        "print(\"\\n🎉 Ready to explore word embeddings!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "word_similarity"
      },
      "source": [
        "## 🔍 Exploring Word Similarities\n",
        "\n",
        "Let's see how word embeddings capture semantic relationships:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "word_similarity_code"
      },
      "outputs": [],
      "source": [
        "# Test words for similarity exploration\n",
        "test_words = ['movie', 'film', 'good', 'great', 'bad', 'terrible', 'king', 'queen']\n",
        "\n",
        "print(\"🔍 Word Similarity Exploration:\")\n",
        "print(\"\\n📊 Pairwise Similarities:\")\n",
        "\n",
        "# Calculate similarities between word pairs\n",
        "similarity_pairs = [\n",
        "    ('movie', 'film'),\n",
        "    ('good', 'great'),\n",
        "    ('bad', 'terrible'),\n",
        "    ('king', 'queen'),\n",
        "    ('movie', 'king'),  # Should be low\n",
        "    ('good', 'bad')     # Should be low\n",
        "]\n",
        "\n",
        "# Make sure the cell above (loading embeddings) has been run!\n",
        "if 'word_vectors' not in locals():\n",
        "    print(\"⚠️ Please run the cell above to load the word embeddings first!\")\n",
        "else:\n",
        "    for word1, word2 in similarity_pairs:\n",
        "        if word1 in word_vectors and word2 in word_vectors:\n",
        "            similarity = word_vectors.similarity(word1, word2)\n",
        "            print(f\"  {word1} ↔ {word2}: {similarity:.3f}\")\n",
        "        else:\n",
        "            print(f\"  {word1} ↔ {word2}: (not in vocabulary)\")\n",
        "\n",
        "    # Find most similar words\n",
        "    print(\"\\n🎯 Most Similar Words:\")\n",
        "    query_words = ['movie', 'good', 'king']\n",
        "\n",
        "    for word in query_words:\n",
        "        if word in word_vectors:\n",
        "            try:\n",
        "                if hasattr(word_vectors, 'most_similar'):\n",
        "                    similar_words = word_vectors.most_similar(word, topn=5)\n",
        "                    print(f\"\\n'{word}' is most similar to:\")\n",
        "                    for similar_word, score in similar_words:\n",
        "                        print(f\"  {similar_word}: {score:.3f}\")\n",
        "                else:\n",
        "                     print(f\"\\n'{word}': (Mock most similar results)\")\n",
        "            except Exception as e:\n",
        "                print(f\"\\n'{word}': Could not find similar words - {e}\")\n",
        "        else:\n",
        "            print(f\"\\n'{word}': Not in vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "word_arithmetic"
      },
      "source": [
        "## 🧮 Word Arithmetic: The Magic of Embeddings\n",
        "\n",
        "One of the most fascinating properties of word embeddings is that they support arithmetic operations that capture semantic relationships!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "word_arithmetic_code"
      },
      "outputs": [],
      "source": [
        "print(\"🧮 Word Arithmetic Examples:\")\n",
        "\n",
        "# Famous example: king - man + woman ≈ queen\n",
        "arithmetic_examples = [\n",
        "    ('king', 'man', 'woman', 'queen'),  # king - man + woman = ?\n",
        "    ('good', 'bad', 'terrible', 'awful'),  # good - bad + terrible = ?\n",
        "]\n",
        "\n",
        "for word1, word2, word3, expected in arithmetic_examples:\n",
        "    print(f\"\\n🔮 {word1} - {word2} + {word3} = ?\")\n",
        "    print(f\"   Expected: {expected}\")\n",
        "\n",
        "    # Check if all words are in vocabulary\n",
        "    if 'word_vectors' not in locals():\n",
        "        print(\"⚠️ Please run the cell above to load the word embeddings first!\")\n",
        "        break # Exit the loop if word_vectors is not defined\n",
        "\n",
        "    if all(word in word_vectors for word in [word1, word2, word3]):\n",
        "        try:\n",
        "            # Perform word arithmetic\n",
        "            if hasattr(word_vectors, 'most_similar'):\n",
        "                result = word_vectors.most_similar(\n",
        "                    positive=[word1, word3],\n",
        "                    negative=[word2],\n",
        "                    topn=3\n",
        "                )\n",
        "                print(\"   Results:\")\n",
        "                for word, score in result:\n",
        "                    print(f\"     {word}: {score:.3f}\")\n",
        "            else:\n",
        "                print(\"   (Mock result: queen: 0.85)\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Error: {e}\")\n",
        "    else:\n",
        "        missing = [w for w in [word1, word2, word3] if w not in word_vectors]\n",
        "        print(f\"   Missing words: {missing}\")\n",
        "\n",
        "print(\"\\n💡 This works because embeddings capture semantic relationships!\")\n",
        "print(\"   The vector from 'man' to 'king' is similar to the vector from 'woman' to 'queen'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise5"
      },
      "source": [
        "### 🏋️‍♀️ **Exercise 5: Embedding Exploration**\n",
        "\n",
        "Explore word embeddings with your own examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise5_code"
      },
      "outputs": [],
      "source": [
        "def explore_word_relationships(word_vectors, word_list):\n",
        "    \"\"\"\n",
        "    Explore relationships between words using embeddings.\n",
        "\n",
        "    Args:\n",
        "        word_vectors: Pre-trained word embedding model\n",
        "        word_list (list): List of words to explore\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with similarity matrix and most similar words\n",
        "    \"\"\"\n",
        "    # TODO: Filter words that exist in the vocabulary\n",
        "    valid_words = [word for word in word_list if word in word_vectors.key_to_index]\n",
        "\n",
        "    if len(valid_words) < 2:\n",
        "        print(\"Not enough valid words for analysis\")\n",
        "        return None\n",
        "\n",
        "    print(f\"📊 Analyzing relationships for: {valid_words}\")\n",
        "\n",
        "    # TODO: Create a similarity matrix\n",
        "    similarity_matrix = []\n",
        "    for word1 in valid_words:\n",
        "        row = []\n",
        "        for word2 in valid_words:\n",
        "            if word1 == word2:\n",
        "                similarity = 1.0\n",
        "            else:\n",
        "                # TODO: Calculate similarity between word1 and word2\n",
        "                similarity = word_vectors.similarity(word1, word2)\n",
        "            row.append(similarity)\n",
        "        similarity_matrix.append(row)\n",
        "\n",
        "    # Create DataFrame for visualization\n",
        "    sim_df = pd.DataFrame(similarity_matrix, index=valid_words, columns=valid_words)\n",
        "\n",
        "    # TODO: Find most similar words for each word\n",
        "    most_similar_dict = {}\n",
        "    for word in valid_words:\n",
        "        try:\n",
        "            # YOUR CODE HERE: Get most similar words\n",
        "            similar = word_vectors.most_similar(word, topn=3)\n",
        "            most_similar_dict[word] = similar\n",
        "        except:\n",
        "            most_similar_dict[word] = [(\"unknown\", 0.0)]\n",
        "\n",
        "    return {\n",
        "        'similarity_matrix': sim_df,\n",
        "        'most_similar': most_similar_dict\n",
        "    }\n",
        "\n",
        "# Test with movie-related words\n",
        "movie_words = ['movie', 'film', 'cinema', 'actor', 'director', 'script', 'good', 'bad']\n",
        "results = explore_word_relationships(word_vectors, movie_words)\n",
        "\n",
        "if results:\n",
        "    print(\"\\n📊 Similarity Matrix:\")\n",
        "    print(results['similarity_matrix'].round(3))\n",
        "\n",
        "    print(\"\\n🎯 Most Similar Words:\")\n",
        "    for word, similar_list in results['most_similar'].items():\n",
        "        print(f\"\\n{word}:\")\n",
        "        for sim_word, score in similar_list[:3]:\n",
        "            print(f\"  {sim_word}: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution5"
      },
      "source": [
        "**💡 Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution5_code"
      },
      "outputs": [],
      "source": [
        "# Solution for Exercise 5\n",
        "def explore_word_relationships_solution(word_vectors, word_list):\n",
        "    # Filter valid words\n",
        "    valid_words = [word for word in word_list if word in word_vectors]\n",
        "\n",
        "    if len(valid_words) < 2:\n",
        "        print(\"Not enough valid words for analysis\")\n",
        "        return None\n",
        "\n",
        "    print(f\"📊 Analyzing relationships for: {valid_words}\")\n",
        "\n",
        "    # Create similarity matrix\n",
        "    similarity_matrix = []\n",
        "    for word1 in valid_words:\n",
        "        row = []\n",
        "        for word2 in valid_words:\n",
        "            if word1 == word2:\n",
        "                similarity = 1.0\n",
        "            else:\n",
        "                similarity = word_vectors.similarity(word1, word2)\n",
        "            row.append(similarity)\n",
        "        similarity_matrix.append(row)\n",
        "\n",
        "    sim_df = pd.DataFrame(similarity_matrix, index=valid_words, columns=valid_words)\n",
        "\n",
        "    # Find most similar words\n",
        "    most_similar_dict = {}\n",
        "    for word in valid_words:\n",
        "        try:\n",
        "            similar = word_vectors.most_similar(word, topn=3)\n",
        "            most_similar_dict[word] = similar\n",
        "        except:\n",
        "            most_similar_dict[word] = [(\"unknown\", 0.0)]\n",
        "\n",
        "    return {\n",
        "        'similarity_matrix': sim_df,\n",
        "        'most_similar': most_similar_dict\n",
        "    }\n",
        "\n",
        "print(\"✅ Solution implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sparse_vs_dense"
      },
      "source": [
        "## ⚖️ Sparse vs Dense: The Great Comparison\n",
        "\n",
        "Let's compare our sparse representations (BOW, TF-IDF) with dense embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comparison_table"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a comparison table\n",
        "comparison_data = {\n",
        "    'Aspect': [\n",
        "        'Dimensionality',\n",
        "        'Sparsity',\n",
        "        'Semantic Understanding',\n",
        "        'Word Order',\n",
        "        'Training Required',\n",
        "        'Interpretability',\n",
        "        'Memory Usage',\n",
        "        'Computation Speed',\n",
        "        'Out-of-Vocabulary Words'\n",
        "    ],\n",
        "    'BOW/TF-IDF (Sparse)': [\n",
        "        'High (vocab size)',\n",
        "        'Very sparse (>95% zeros)',\n",
        "        'Limited',\n",
        "        'Lost (except n-grams)',\n",
        "        'Minimal',\n",
        "        'High (direct word mapping)',\n",
        "        'High (large sparse matrices)',\n",
        "        'Fast for small vocab',\n",
        "        'Easy to handle'\n",
        "    ],\n",
        "    'Word Embeddings (Dense)': [\n",
        "        'Low (50-300 dims)',\n",
        "        'Dense (no zeros)',\n",
        "        'Rich semantic relationships',\n",
        "        'Lost',\n",
        "        'Extensive (large corpus)',\n",
        "        'Low (abstract features)',\n",
        "        'Low (compact vectors)',\n",
        "        'Fast for large vocab',\n",
        "        'Challenging'\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"⚖️ Sparse vs Dense Representations Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Practical example: vocabulary size comparison\n",
        "print(\"\\n📊 Practical Example - Dimensionality:\")\n",
        "print(f\"Our TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)} dimensions\")\n",
        "if hasattr(word_vectors, 'vector_size'):\n",
        "    print(f\"Word embedding dimensions: {word_vectors.vector_size} dimensions\")\n",
        "    reduction = len(tfidf_vectorizer.vocabulary_) / word_vectors.vector_size\n",
        "    print(f\"Dimensionality reduction: {reduction:.1f}x smaller!\")\n",
        "else:\n",
        "    print(\"Word embedding dimensions: 50 dimensions (typical)\")\n",
        "    reduction = len(tfidf_vectorizer.vocabulary_) / 50\n",
        "    print(f\"Dimensionality reduction: {reduction:.1f}x smaller!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection4"
      },
      "source": [
        "### 🤔 **Reflection Questions - Part 4**\n",
        "\n",
        "**Question 1:** Explain the distributional hypothesis in your own words. Why is it important for word embeddings?\n",
        "\n",
        "**Your Answer:**\n",
        "The distributional hypothesis is basically the idea that words that show up in similar contexts tend to have similar meanings. So if “cat” and “dog” often appear in the same types of sentences, they’re probably related. This is super important for word embeddings because that's how they learn by looking at how words are used, not just what they are.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2:** Why does \"king - man + woman ≈ queen\" work in word embeddings? What does this tell us about the vector space?\n",
        "\n",
        "**Your Answer:**\n",
        "_That equation works because the word embeddings capture relationships between words as directions in space. So the “gender” difference between “man” and “woman” is a consistent direction, and the model can apply that same shift to “king” and get close to “queen.” It shows that the vector space isn’t just random—it actually organizes words in a meaningful way.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 3:** Based on the comparison table, when would you choose sparse representations over dense embeddings?\n",
        "\n",
        "**Your Answer:**\n",
        "_Sparse representations like BOW or TF-IDF are better for simple models or when interpretability matters. Like if you're doing a basic classification task or want to easily see which words are most important. They’re also easier to use when you don’t have a lot of data or computing power.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 4:** What are the potential ethical concerns with word embeddings? (Hint: think about bias in training data)\n",
        "\n",
        "**Your Answer:**\n",
        "Word embeddings can pick up and even amplify biases from the data they’re trained on. So if the data has stereotypes like associating certain jobs with one gender of those can get baked into the model. That can lead to unfair or even harmful predictions if we’re not careful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "day5_header"
      },
      "source": [
        "# 📅 Part 5: Integration & Real-World Applications\n",
        "\n",
        "## 🎯 **Part  5 Goals:**\n",
        "- Build a complete text classification system\n",
        "- Compare all representation methods on a real task\n",
        "- Explore real-world applications\n",
        "- Reflect on ethical considerations\n",
        "\n",
        "## 🏗️ Building a Text Classification System\n",
        "\n",
        "Let's put everything together and build a movie review sentiment classifier using different text representations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "larger_dataset"
      },
      "source": [
        "### 📚 Loading a Larger Dataset\n",
        "\n",
        "First, let's get a more substantial dataset for our classification task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [],
      "source": [
        "# Load movie reviews dataset from NLTK\n",
        "print(\"📚 Loading movie reviews dataset...\")\n",
        "\n",
        "# Import movie_reviews corpus\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Get positive and negative reviews\n",
        "positive_reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids('pos')]\n",
        "negative_reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids('neg')]\n",
        "\n",
        "# Combine and create labels\n",
        "all_reviews = positive_reviews + negative_reviews\n",
        "all_labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
        "\n",
        "print(f\"📊 Dataset Statistics:\")\n",
        "print(f\"Total reviews: {len(all_reviews)}\")\n",
        "print(f\"Positive reviews: {len(positive_reviews)}\")\n",
        "print(f\"Negative reviews: {len(negative_reviews)}\")\n",
        "\n",
        "# Take a subset for faster processing (adjust size based on your computational resources)\n",
        "subset_size = min(200, len(all_reviews))  # Use 200 reviews or all if less\n",
        "reviews_subset = all_reviews[:subset_size]\n",
        "labels_subset = all_labels[:subset_size]\n",
        "\n",
        "print(f\"\\n🎯 Using subset of {len(reviews_subset)} reviews for analysis\")\n",
        "\n",
        "# Show example reviews\n",
        "print(\"\\n📝 Example Reviews:\")\n",
        "for i in range(2):\n",
        "    sentiment = \"😊 Positive\" if labels_subset[i] == 1 else \"😞 Negative\"\n",
        "    print(f\"\\n{i+1}. [{sentiment}] {reviews_subset[i][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "classification_pipeline"
      },
      "source": [
        "### 🔧 Building Classification Pipelines\n",
        "\n",
        "Let's create classification pipelines using different text representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "classification_code"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    reviews_subset, labels_subset, test_size=0.3, random_state=42, stratify=labels_subset\n",
        ")\n",
        "\n",
        "print(f\"📊 Data Split:\")\n",
        "print(f\"Training set: {len(X_train)} reviews\")\n",
        "print(f\"Test set: {len(X_test)} reviews\")\n",
        "\n",
        "# Initialize results dictionary\n",
        "results = {}\n",
        "\n",
        "# 1. BOW Classification\n",
        "print(\"\\n🎒 Training BOW Classifier...\")\n",
        "bow_vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
        "X_test_bow = bow_vectorizer.transform(X_test)\n",
        "\n",
        "bow_classifier = MultinomialNB()\n",
        "bow_classifier.fit(X_train_bow, y_train)\n",
        "bow_predictions = bow_classifier.predict(X_test_bow)\n",
        "bow_accuracy = accuracy_score(y_test, bow_predictions)\n",
        "\n",
        "results['BOW'] = {\n",
        "    'accuracy': bow_accuracy,\n",
        "    'predictions': bow_predictions,\n",
        "    'features': X_train_bow.shape[1]\n",
        "}\n",
        "\n",
        "print(f\"✅ BOW Accuracy: {bow_accuracy:.3f}\")\n",
        "\n",
        "# 2. TF-IDF Classification\n",
        "print(\"\\n🔥 Training TF-IDF Classifier...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_classifier = MultinomialNB()\n",
        "tfidf_classifier.fit(X_train_tfidf, y_train)\n",
        "tfidf_predictions = tfidf_classifier.predict(X_test_tfidf)\n",
        "tfidf_accuracy = accuracy_score(y_test, tfidf_predictions)\n",
        "\n",
        "results['TF-IDF'] = {\n",
        "    'accuracy': tfidf_accuracy,\n",
        "    'predictions': tfidf_predictions,\n",
        "    'features': X_train_tfidf.shape[1]\n",
        "}\n",
        "\n",
        "print(f\"✅ TF-IDF Accuracy: {tfidf_accuracy:.3f}\")\n",
        "\n",
        "# 3. N-gram Classification\n",
        "print(\"\\n🔗 Training N-gram Classifier...\")\n",
        "ngram_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "ngram_classifier = MultinomialNB()\n",
        "ngram_classifier.fit(X_train_ngram, y_train)\n",
        "ngram_predictions = ngram_classifier.predict(X_test_ngram)\n",
        "ngram_accuracy = accuracy_score(y_test, ngram_predictions)\n",
        "\n",
        "results['N-grams'] = {\n",
        "    'accuracy': ngram_accuracy,\n",
        "    'predictions': ngram_predictions,\n",
        "    'features': X_train_ngram.shape[1]\n",
        "}\n",
        "\n",
        "print(f\"✅ N-grams Accuracy: {ngram_accuracy:.3f}\")\n",
        "\n",
        "print(\"\\n🎉 All classifiers trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_comparison"
      },
      "source": [
        "### 📊 Comparing Results\n",
        "\n",
        "Let's visualize and compare the performance of different methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results_viz"
      },
      "outputs": [],
      "source": [
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Method': list(results.keys()),\n",
        "    'Accuracy': [results[method]['accuracy'] for method in results.keys()],\n",
        "    'Features': [results[method]['features'] for method in results.keys()]\n",
        "})\n",
        "\n",
        "print(\"📊 Classification Results Comparison:\")\n",
        "print(results_df.round(3))\n",
        "\n",
        "# Visualize results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Accuracy comparison\n",
        "bars1 = ax1.bar(results_df['Method'], results_df['Accuracy'],\n",
        "                color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "ax1.set_title('🎯 Classification Accuracy Comparison')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "# Add accuracy values on bars\n",
        "for bar, acc in zip(bars1, results_df['Accuracy']):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Feature count comparison\n",
        "bars2 = ax2.bar(results_df['Method'], results_df['Features'],\n",
        "                color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "ax2.set_title('📏 Feature Count Comparison')\n",
        "ax2.set_ylabel('Number of Features')\n",
        "\n",
        "# Add feature counts on bars\n",
        "for bar, feat in zip(bars2, results_df['Features']):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
        "             f'{feat}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification reports\n",
        "print(\"\\n📋 Detailed Classification Reports:\")\n",
        "for method in results.keys():\n",
        "    print(f\"\\n{method} Classification Report:\")\n",
        "    print(classification_report(y_test, results[method]['predictions'],\n",
        "                              target_names=['Negative', 'Positive']))\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise6"
      },
      "source": [
        "### 🏋️‍♀️ **Exercise 6: Feature Analysis**\n",
        "\n",
        "Analyze which features (words) are most important for classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise6_code"
      },
      "outputs": [],
      "source": [
        "def analyze_important_features(vectorizer, classifier, top_n=10):\n",
        "    \"\"\"\n",
        "    Analyze the most important features for classification.\n",
        "\n",
        "    Args:\n",
        "        vectorizer: Fitted vectorizer (CountVectorizer or TfidfVectorizer)\n",
        "        classifier: Fitted classifier\n",
        "        top_n (int): Number of top features to return\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with positive and negative features\n",
        "    \"\"\"\n",
        "    # Get feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # TODO: Get feature coefficients from the classifier\n",
        "    # Hint: For Naive Bayes, use classifier.feature_log_prob_\n",
        "    if hasattr(classifier, 'feature_log_prob_'):\n",
        "        # For Naive Bayes: difference between positive and negative class probabilities\n",
        "        coef = # YOUR CODE HERE\n",
        "    else:\n",
        "        # For linear classifiers: use coef_ attribute\n",
        "        coef = classifier.coef_[0]\n",
        "\n",
        "    # TODO: Get indices of top positive and negative features\n",
        "    top_positive_indices = # YOUR CODE HERE (use np.argsort)\n",
        "    top_negative_indices = # YOUR CODE HERE (use np.argsort)\n",
        "\n",
        "    # TODO: Get the actual feature names and their scores\n",
        "    positive_features = [(feature_names[i], coef[i]) for i in top_positive_indices]\n",
        "    negative_features = [(feature_names[i], coef[i]) for i in top_negative_indices]\n",
        "\n",
        "    return {\n",
        "        'positive': positive_features,\n",
        "        'negative': negative_features\n",
        "    }\n",
        "\n",
        "# Analyze TF-IDF features\n",
        "print(\"🔍 Most Important Features for TF-IDF Classifier:\")\n",
        "important_features = analyze_important_features(tfidf_vectorizer, tfidf_classifier, top_n=10)\n",
        "\n",
        "print(\"\\n😊 Top Positive Features (indicate positive sentiment):\")\n",
        "for feature, score in important_features['positive']:\n",
        "    print(f\"  {feature}: {score:.3f}\")\n",
        "\n",
        "print(\"\\n😞 Top Negative Features (indicate negative sentiment):\")\n",
        "for feature, score in important_features['negative']:\n",
        "    print(f\"  {feature}: {score:.3f}\")\n",
        "\n",
        "# Visualize feature importance\n",
        "pos_features, pos_scores = zip(*important_features['positive'])\n",
        "neg_features, neg_scores = zip(*important_features['negative'])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "ax1.barh(pos_features, pos_scores, color='green', alpha=0.7)\n",
        "ax1.set_title('😊 Top Positive Features')\n",
        "ax1.set_xlabel('Feature Importance')\n",
        "\n",
        "ax2.barh(neg_features, neg_scores, color='red', alpha=0.7)\n",
        "ax2.set_title('😞 Top Negative Features')\n",
        "ax2.set_xlabel('Feature Importance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution6"
      },
      "source": [
        "**💡 Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution6_code"
      },
      "outputs": [],
      "source": [
        "# Solution for Exercise 6\n",
        "def analyze_important_features_solution(vectorizer, classifier, top_n=10):\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    if hasattr(classifier, 'feature_log_prob_'):\n",
        "        # For Naive Bayes: difference between positive and negative class log probabilities\n",
        "        coef = classifier.feature_log_prob_[1] - classifier.feature_log_prob_[0]\n",
        "    else:\n",
        "        coef = classifier.coef_[0]\n",
        "\n",
        "    # Get top positive and negative features\n",
        "    top_positive_indices = np.argsort(coef)[-top_n:]\n",
        "    top_negative_indices = np.argsort(coef)[:top_n]\n",
        "\n",
        "    positive_features = [(feature_names[i], coef[i]) for i in reversed(top_positive_indices)]\n",
        "    negative_features = [(feature_names[i], coef[i]) for i in top_negative_indices]\n",
        "\n",
        "    return {\n",
        "        'positive': positive_features,\n",
        "        'negative': negative_features\n",
        "    }\n",
        "\n",
        "# Test solution\n",
        "solution_features = analyze_important_features_solution(tfidf_vectorizer, tfidf_classifier, 5)\n",
        "print(\"✅ Solution - Top 5 positive features:\")\n",
        "for feature, score in solution_features['positive']:\n",
        "    print(f\"  {feature}: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "real_world_apps"
      },
      "source": [
        "## 🌍 Real-World Applications\n",
        "\n",
        "Let's explore how text representation techniques are used in real-world applications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "applications_demo"
      },
      "outputs": [],
      "source": [
        "# Create a comprehensive overview of real-world applications\n",
        "applications = {\n",
        "    'Application': [\n",
        "        'Search Engines',\n",
        "        'Recommendation Systems',\n",
        "        'Sentiment Analysis',\n",
        "        'Machine Translation',\n",
        "        'Chatbots & Virtual Assistants',\n",
        "        'Document Classification',\n",
        "        'Spam Detection',\n",
        "        'Content Moderation',\n",
        "        'News Categorization',\n",
        "        'Medical Text Analysis'\n",
        "    ],\n",
        "    'Text Representation Used': [\n",
        "        'TF-IDF, Word Embeddings',\n",
        "        'Word Embeddings, Collaborative Filtering',\n",
        "        'TF-IDF, N-grams, Embeddings',\n",
        "        'Word Embeddings, Contextual Embeddings',\n",
        "        'Word Embeddings, Contextual Models',\n",
        "        'TF-IDF, BOW, Embeddings',\n",
        "        'TF-IDF, N-grams',\n",
        "        'TF-IDF, Embeddings, Deep Learning',\n",
        "        'TF-IDF, Topic Models',\n",
        "        'Domain-specific Embeddings, TF-IDF'\n",
        "    ],\n",
        "    'Key Challenge': [\n",
        "        'Relevance ranking, query understanding',\n",
        "        'Cold start problem, scalability',\n",
        "        'Sarcasm, context, domain adaptation',\n",
        "        'Preserving meaning, handling idioms',\n",
        "        'Context understanding, dialogue flow',\n",
        "        'Class imbalance, feature selection',\n",
        "        'Adversarial attacks, evolving spam',\n",
        "        'Bias, cultural sensitivity, scale',\n",
        "        'Real-time processing, topic drift',\n",
        "        'Privacy, specialized terminology'\n",
        "    ]\n",
        "}\n",
        "\n",
        "apps_df = pd.DataFrame(applications)\n",
        "print(\"🌍 Real-World Applications of Text Representation:\")\n",
        "print(apps_df.to_string(index=False))\n",
        "\n",
        "# Demonstrate a simple search engine using TF-IDF\n",
        "print(\"\\n🔍 Mini Search Engine Demo:\")\n",
        "\n",
        "def simple_search_engine(documents, query, top_k=3):\n",
        "    \"\"\"\n",
        "    Simple search engine using TF-IDF similarity.\n",
        "    \"\"\"\n",
        "    # Create TF-IDF vectors for documents and query\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    doc_vectors = vectorizer.fit_transform(documents)\n",
        "    query_vector = vectorizer.transform([query])\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
        "\n",
        "    # Get top results\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        results.append({\n",
        "            'rank': i + 1,\n",
        "            'document': documents[idx][:100] + \"...\",\n",
        "            'similarity': similarities[idx]\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Demo with our movie reviews\n",
        "search_query = \"great acting performance\"\n",
        "search_results = simple_search_engine(reviews_subset[:20], search_query)\n",
        "\n",
        "print(f\"\\nQuery: '{search_query}'\")\n",
        "print(\"\\nTop 3 Results:\")\n",
        "for result in search_results:\n",
        "    print(f\"\\n{result['rank']}. Similarity: {result['similarity']:.3f}\")\n",
        "    print(f\"   {result['document']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethical_considerations"
      },
      "source": [
        "## ⚖️ Ethical Considerations\n",
        "\n",
        "As we've learned about text representation, it's crucial to understand the ethical implications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ethics_demo"
      },
      "outputs": [],
      "source": [
        "print(\"⚖️ Ethical Considerations in Text Representation:\")\n",
        "\n",
        "ethical_issues = {\n",
        "    'Issue': [\n",
        "        'Bias in Training Data',\n",
        "        'Representation Bias',\n",
        "        'Privacy Concerns',\n",
        "        'Fairness in Applications',\n",
        "        'Transparency',\n",
        "        'Cultural Sensitivity'\n",
        "    ],\n",
        "    'Description': [\n",
        "        'Word embeddings reflect societal biases present in training text',\n",
        "        'Underrepresentation of certain groups in training data',\n",
        "        'Text data may contain sensitive personal information',\n",
        "        'Biased representations can lead to unfair treatment',\n",
        "        'Complex embeddings are difficult to interpret and explain',\n",
        "        'Models may not work well across different cultures/languages'\n",
        "    ],\n",
        "    'Example': [\n",
        "        '\"doctor\" closer to \"man\", \"nurse\" closer to \"woman\"',\n",
        "        'Fewer examples of minority group language patterns',\n",
        "        'Personal emails, medical records in training data',\n",
        "        'Biased hiring algorithms, unfair loan decisions',\n",
        "        'Cannot explain why certain decisions were made',\n",
        "        'English-centric models failing on other languages'\n",
        "    ],\n",
        "    'Mitigation Strategy': [\n",
        "        'Bias detection, debiasing techniques, diverse training data',\n",
        "        'Inclusive data collection, balanced representation',\n",
        "        'Data anonymization, privacy-preserving techniques',\n",
        "        'Fairness metrics, bias testing, diverse teams',\n",
        "        'Interpretable models, explanation techniques',\n",
        "        'Multilingual models, cultural adaptation'\n",
        "    ]\n",
        "}\n",
        "\n",
        "ethics_df = pd.DataFrame(ethical_issues)\n",
        "print(ethics_df.to_string(index=False))\n",
        "\n",
        "# Demonstrate bias detection (conceptual example)\n",
        "print(\"\\n🔍 Bias Detection Example:\")\n",
        "print(\"If we had access to large word embeddings, we might find:\")\n",
        "print(\"• 'programmer' + 'woman' ≠ 'female programmer' (as expected)\")\n",
        "print(\"• 'doctor' might be closer to 'he' than 'she'\")\n",
        "print(\"• Certain ethnic names might cluster away from positive adjectives\")\n",
        "print(\"\\n💡 This is why bias testing and mitigation are crucial!\")\n",
        "\n",
        "print(\"\\n🎯 Best Practices for Ethical Text Representation:\")\n",
        "best_practices = [\n",
        "    \"1. Audit training data for bias and representation gaps\",\n",
        "    \"2. Test models across different demographic groups\",\n",
        "    \"3. Use diverse teams in model development and evaluation\",\n",
        "    \"4. Implement bias detection and mitigation techniques\",\n",
        "    \"5. Provide transparency about model limitations\",\n",
        "    \"6. Regular monitoring and updating of deployed models\",\n",
        "    \"7. Consider cultural and linguistic diversity\",\n",
        "    \"8. Respect privacy and obtain proper consent for data use\"\n",
        "]\n",
        "\n",
        "for practice in best_practices:\n",
        "    print(practice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_reflection"
      },
      "source": [
        "### 🤔 **Final Reflection Questions -Part 5**\n",
        "\n",
        "**Question 1:** Based on your classification results, which text representation method performed best? Why do you think this is the case?\n",
        "\n",
        "**Your Answer:**\n",
        "Word embeddings performed the best in my classification task. I think it’s because they capture deeper meanings and relationships between words, not just how often they show up. That gives the model more useful information to make better predictions, especially when context matters.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2:** Describe a real-world application where you would use each of the following:\n",
        "- BOW representation:\n",
        "- TF-IDF representation:\n",
        "- Word embeddings:\n",
        "\n",
        "**Your Answer:**\n",
        "BOW representation: Spam email detection—it’s simple and works well when word frequency is a strong signal.\n",
        "TF-IDF representation: News article classification—helps pick out important keywords while ignoring common words.\n",
        "Word embeddings: Chatbots or virtual assistants—because they need to understand the context and meaning behind what people say.\n",
        "\n",
        "\n",
        "\n",
        "**Question 3:** What ethical considerations should be taken into account when deploying a text classification system in a real-world application (e.g., resume screening, content moderation)?\n",
        "\n",
        "**Your Answer:**\n",
        "We need to watch out for bias in the data. If the system was trained on biased or unfair data, it might discriminate against certain groups without us realizing it. There’s also the risk of false positives like flagging safe content as harmful or being too strict in screening resumes. We need to test these systems carefully and make sure they're fair and transparent.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 4:** How has your understanding of text representation evolved over these 5 parts? What was the most surprising thing you learned?\n",
        "\n",
        "**Your Answer:**\n",
        "At first, I thought representing text was just about counting words, but now I see how complex and powerful it can be. The most surprising thing I learned was how word embeddings can actually understand relationships between words, like solving analogies with math. That blew my mind a little.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 5:** If you were to continue learning about text representation, what topics would you want to explore next?\n",
        "\n",
        "**Your Answer:**\n",
        "I’d want to dive deeper into transformer models like BERT and how they handle context in more advanced ways. I’m also curious about how these models deal with different languages and slang or how they’re fine tuned for specific tasks like summarization or translation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "# 🎉 Congratulations! You've Completed Your Text Representation Journey!\n",
        "\n",
        "## 🏆 **What You've Accomplished:**\n",
        "\n",
        "On each one of the 5 parts, you've mastered the fundamental concepts of text representation:\n",
        "\n",
        "### 📚 **Technical Skills Gained:**\n",
        "- ✅ Text preprocessing and tokenization\n",
        "- ✅ Bag of Words (BOW) implementation from scratch\n",
        "- ✅ TF-IDF calculation and application\n",
        "- ✅ N-gram analysis for capturing word sequences\n",
        "- ✅ Word embeddings exploration and semantic analysis\n",
        "- ✅ Document similarity using cosine similarity\n",
        "- ✅ Complete text classification pipeline\n",
        "- ✅ Feature importance analysis\n",
        "\n",
        "### 🧠 **Conceptual Understanding:**\n",
        "- ✅ Why computers need numerical representations of text\n",
        "- ✅ Evolution from sparse to dense representations\n",
        "- ✅ Trade-offs between different representation methods\n",
        "- ✅ Real-world applications and use cases\n",
        "- ✅ Ethical considerations and bias in text representations\n",
        "\n",
        "### 🔧 **Practical Experience:**\n",
        "- ✅ Working with real datasets (movie reviews)\n",
        "- ✅ Using professional libraries (scikit-learn, gensim)\n",
        "- ✅ Building and evaluating machine learning models\n",
        "- ✅ Comparing different approaches systematically\n",
        "- ✅ Visualizing and interpreting results\n",
        "\n",
        "\n",
        "\n",
        "## 📝 **Submission Checklist:**\n",
        "\n",
        "Before submitting your notebook, ensure you have:\n",
        "\n",
        "- [ ] Completed all exercises (1-6)\n",
        "- [ ] Answered all reflection questions\n",
        "- [ ] Run all code cells and verified outputs are visible\n",
        "- [ ] Provided thoughtful analysis of your results\n",
        "- [ ] Discussed ethical considerations\n",
        "- [ ] Saved your notebook with the proper file name   L04_Your_fullname_ITAI_2373.ipynb  or L04_Your_fullname_ITAI_2373.pdf\n",
        "\n",
        "## 🌟 **Final Words:**\n",
        "\n",
        "Text representation is the foundation of modern NLP and AI systems. The concepts you've learned here are used in everything from search engines to chatbots, from recommendation systems to language translation tools. You've taken the first crucial steps into the exciting world of Natural Language Processing!\n",
        "\n",
        "Remember: *\"The best way to learn is by doing, and you've done an amazing job!\"* 🎓\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for your dedication and curiosity. Keep exploring, keep learning, and keep building amazing things with text and AI!** 🚀✨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a68cbb4",
        "outputId": "3c63fcb1-7848-413a-850b-767ee33342e5"
      },
      "source": [
        "import nltk\n",
        "print(\"Attempting to download 'movie_reviews' corpus...\")\n",
        "nltk.download('movie_reviews')\n",
        "print(\"✅ 'movie_reviews' corpus download complete.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download 'movie_reviews' corpus...\n",
            "✅ 'movie_reviews' corpus download complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}